+++
categories = ["Python"]
isCJKLanguage = true
topics = ["Python"]
draft = false
type = "post"

title  = "日志处理的一些心得"
date = "2015-06-09T19:32:40+08:00"
+++

今天花了很长时间处理日志，然而并没有什么卵用。

发给我的日志是个压缩包，127M，解压后大概是430多M，在我之前的工作生涯中是没有见过那么大的日志的，果然在小公司待的时间长了是完全体会不到大公司的量级了。

言归正传，说说问题本身。

1. 日志有6天的纪录，4列数据，一定是我前面已经有人筛选过了，不过ta画蛇添足的把这几天的日志merge到一起了，我还得再给他拆分。
2. 第一列是日期，第二列是来源，第三列是重点，是object_id，需要拿这个id作为HTTP请求的参数，然后根据返回的结果求和第四列的值。

条件是这样的：
需要调用的接口是可以接受批处理请求的，而且一次接受的id数量不能超过100个。

因此，我起初最简单的想法就是

1. 只要文件还没有结束，就顺序读取文件的每一行，当读取的行数是100的倍数时，就把这最近的100个id组合成一个参数值传到请求的url参数表中
2. 从返回的结果中查找想要的字段
3. 根据该字段的值对第四列的次数做求和

本身是直观的一个想法，但实际上实施起来就没那么简单了，对，就是内存溢出。

看到这个结果首先想到的就是拆分文件，但实际上我按天拆分了之后完全起不到作用，日志总共有700多万条，而即使按天拆分每天也是有100多万条，这100多万条本身并不大，但如果算上每100条就要请求发一次HTTP请求，而且要根据返回值进行一系列的判断的话，内存就吃紧了，要知道我不是在那仅仅有1G内存的公司开发用的服务器上跑的程序，而是在我新买的16G内存的rMBP 15'上跑的！

所以就还需要想办法，我就想到了之前的公司经常用的`shell_exec` 打法，但整理了一下思路之后发现这个方法的实施难度也是挺高的，因为这并不是数据库操作，如果是数据库的话，可以给要执行的worker [^worker] 部分的代码传入一个`limit` ，然后它就根据这个值自动的查询到你要的结果，但这里不是，一旦你读入了这个文件，就要一直读下去，中途如果`close`掉了，就再也不知道上一次运行到哪里了，那能它传入什么呢？传入HTTP请求的返回值？如果这个HTTP请求都已经放在invoker里面了，那其实worker就没有存在的必要了。无论如何我也想不出这个方案的可行度，后来放弃了。

然后呢，需求方那边要得急，我也来不及多想，只能想着把文件再切分，直接上了`split`方法，把文件切分成每10万行一个的小文件，为了加快速度，把文件作为参数传入脚本，开很多个终端同时跑，出来的结果是多行，但列数是固定的，这时就可以用awk把最终的结果搞出来

```sh
split -l 100000 all.log stat_
```

简单说一下split的用法，`-l`指定每个文件的大小，比如这个文件一共750000行，那你每10万行分成一个文件，前7个文件都是100000行，最后一个当然就只有50000万行。
最后一个参数是生成的文件的前缀，比如执行完这条命令后会在当前目录里生成若干个`stat_aa`之类的文件。这是默认的情况，其实是可以指定用来区分每个文件的后缀的长度的，它是这个命令的第一个参数，不可以放在最后，`-a 4`，在`split`后紧跟这个命令得到的文件名就形如`stat_aaaa`这样的了。

```sh
awk -F'|' 'BEGIN{sum=0}{sum+=$1}END{print sum}' $1
echo '|'
```

上面这行是第一列的和，多写几遍就得到了类似
`x1 | x2 | x3 | ..`的一行数据，直接粘在markdown编辑器里就能得到他们要得表格了。

虽然这个方法土的掉渣，朴素的掉渣，但在短时间内确实解决了问题，也不枉我这一下午的时间。但我心里还是觉得难受，这样的工作其实是没有什么意义的，解决这种大数据量的问题，让计算机自己去分片然后处理才是王道，这样的做法虽然是解决了问题，但问题其实是仍然存在的。

下面多想一点，能不能把我自己手工做的工作让计算机自己去做呢？

1. 分片，已经确定了分成10万行的单个文件处理起来是没有问题的
2. 把文件作为参数传入脚本，这就牵涉到一个for循环

脚本的执行流程就可以是这样

1. 先分片
2. 在invoker中组织`stat_aaaa`这样的文件名，把文件传入用`shell_exec`调用的命令中作为参数，命令直接输出到标准输出，或者复杂一点可以写入文件，其实是写入文件更方便操作。
3. 在worker中执行完之后`exit`，内存释放，重新回到invoker
4. invoker把下一个文件名传给worker，重复3，直至所有文件遍历完毕。

明天可以抽时间实践一下这个想法。

2015年6月10日更新：
今天尝试了一下昨天想到的方案，果然好用，再加一点，把每个worker的输出结果重定向到一个结果文件中，最终就可以直接出来结果文件了，很简单了。我真是太有才了。

不过这其实也不是一个优雅的方案，这仅仅是用了PHP内建的cURL功能来完成网络请求，主要用的还是UNIX得那一套工具链，并没有把工具很好的集成。

[^worker]: 和invoker都是我自己的说法，其中invoker指的是入口脚本，在它里面做数据分片的工作，实际的工作在worker中完成，然后退出，控制权回到invoker继续调用下一次worker，达到每做完一个worker的工作，其占用的内存就释放掉的效果。
